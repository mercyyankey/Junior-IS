% Here is an example of how to create a bibliography entry for an article using
% BibTeX. Generally you won't have to write these out yourself, because they are
% provided by most web sites that allow you to export citations. The string
% "clrsAlgorithms" is a citation key, and if you were citing the source in a
% document you would use \cite{clrsAlgorithms}.
@article{dayanand2025irula,
author = {Dayanand, Arul and Devi, Uma and Jinu, Kevin Thomas and Siddiqui, Mohammed Ariz and Thekkudan, Evina Novi},
title = {Revitalising the Irula Language Through Mobile Language Learning Application},
journal = {Cureus Journal of Computer Science},
year = {2025},
doi = {10.7759/s44389-025-08635-7},
annote = {Revitalising the Irula Language Through Mobile Language Learning Application examines the use of a mobile-assisted language learning (MALL) system to support the preservation of the endangered Irula language. The authors identify language shift, social discrimination, and declining interest among younger generations as the primary causes of the language’s endangerment. To address this issue, the study focuses on the design and development of an Android-based language learning application intended to promote autonomous learning and cultural engagement.

The authors employ a mixed-methods approach, beginning with the collection of secondary language data and following an Agile software development paradigm. The program features a multilingual English–Tamil interface and is developed on React Native to facilitate cross-platform deployment. Audio pronunciation, sample sentences, and grammatical explanations are essential components that support learners in making judgments during vocabulary learning and practice.

The system is evaluated through exploratory testing on six Android and iOS devices to assess performance and responsiveness. An optimized search algorithm with delay functionality reduces latency and improves usability. Based on Technology Acceptance Model (TAM) principles, user feedback indicates that the audiovisual elements and interface design are well received, while also identifying areas for future improvement. This source is relevant to research on decision-making in language learning systems because it demonstrates how computational design choices and evaluation metrics influence learner engagement, accessibility, and system effectiveness.},
}
@article{chan2025xrmall,
  author = {Chan, Carol K. K. and Wong, Lily Y. M.},
  title = {Investigating the Effects of Extended Reality (XR) on Interpreter Competencies: An Experimental Study of a Pioneering XR Mobile-Assisted Language Learning Application for Interpreting Training},
  journal = {Journal of Computer Assisted Learning},
  year = {2025},
  doi = {10.1111/jcal.13014},
  annote = {Investigating the Effects of Extended Reality (XR) on Interpreter Competencies: An Experimental Study of a Pioneering XR Mobile-Assisted Language Learning Application for Interpreting Training explores how extended reality technology can enhance interpreter training through a mobile-assisted language learning (MALL) system. The authors argue that real-world situations and visual cues are essential for developing professional interpreting competencies, yet are often absent from traditional classroom-based instruction. To address this limitation, the study presents an XR-based mobile learning application designed to support blended and immersive learning environments.


The authors employ a quasi-experimental research design involving an experimental group and a control group over a ten-week period. The XR MALL system integrates learning resources, video demonstrations, virtual reality (VR) practice, and augmented reality (AR) vocabulary features, enabling learners to engage in immersive and interactive interpreting scenarios. Learners’ interpreting knowledge and practical skills are evaluated using pre- and post-study interpreting tasks, allowing the system to track changes in performance over time.




The findings indicate that participants in the XR MALL program demonstrate significantly greater improvements in both declarative knowledge of interpreting and practical interpreting techniques than the control group. This source is relevant to research on decision-making in language learning systems because it demonstrates how computational learning environments can influence learner performance, guide skill development, and evaluate competency through structured experimental assessment methods.}
}

@inproceedings{hug2025ailiteracy,
  author = {Hug, Sarah and McKay, Mark},
  title = {Problematizing {AI} Literacy Access: Understanding Student {AI} Literacy from Student Voices},
  booktitle = {Proceedings of the 2025 Conference on Research on Equitable and Sustained Participation in Engineering, Computing, and Technology},
  series = {RESPECT 2025},
  year = {2025},
  pages = {339--342},
  location = {Newark, NJ, USA},
  publisher = {Association for Computing Machinery},
  doi = {10.1145/3704637.3734789},
  url = {https://doi.org/10.1145/3704637.3734789},
  annote = {Problematizing AI Literacy Access: Understanding Student AI Literacy from Student Voices analyzes how low-income undergraduate computer science students understand, access, and make decisions regarding the use of generative artificial intelligence tools in academic settings. The authors focus on Pell Grant–eligible students attending open-access institutions and argue that institutional guidance and formal instruction have not kept pace with the rapid adoption of tools such as ChatGPT. As a result, AI literacy is framed not only as technical understanding, but also as an issue of access, judgment, and informed decision-making within computing education.

The study employs a qualitative research methodology based on interviews with undergraduate students to examine participants’ attitudes toward generative AI technologies for academic purposes. Through analysis of student narratives, the authors investigate how learners assess the applicability, accuracy, and usefulness of AI systems when completing coursework. The findings indicate that many students rely on personal judgment and unstructured experimentation rather than formal instructional frameworks when deciding how and when to use AI tools.

The results further demonstrate that inconsistent institutional messaging contributes to confusion in student decision-making and highlights significant disparities in access to AI literacy. This source is relevant to research on decision-making in computational systems because it illustrates how user judgment, educational context, and system accessibility shape interactions with AI technologies, emphasizing the need for transparent and equitable approaches to AI education.}
}

  


